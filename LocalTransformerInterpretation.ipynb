{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.python.keras.layers.merge import concatenate\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfa.__version__)\n",
    "print(tfp.__version__)\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from os import walk\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import arff\n",
    "\n",
    "from pyts.approximation import SymbolicAggregateApproximation\n",
    "\n",
    "from modules import helper\n",
    "from modules import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random variables so that one run on the same computer always results in the same models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.RandomState(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "context.set_global_seed(seed_value)\n",
    "ops.get_default_graph().seed = seed_value\n",
    "\n",
    "#pip install tensorflow-determinism needed\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and formarting the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset!!!\n",
    "\n",
    "#source: http://www.timeseriesclassification.com/description.php?Dataset=SyntheticControl\n",
    "data_path_train = './Datasets/SyntheticControl/SyntheticControl_TRAIN.arff'\n",
    "data_path_test = './Datasets/SyntheticControl/SyntheticControl_TEST.arff'\n",
    "num_of_classes = 6\n",
    "seqSize = 60\n",
    "\n",
    "#source: http://www.timeseriesclassification.com/description.php?Dataset=ECG5000\n",
    "#data_path_train = './Datasets/ecg5000/ECG5000_TRAIN.arff'\n",
    "#data_path_test = './Datasets/ecg5000/ECG5000_TEST.arff'\n",
    "#num_of_classes = 5\n",
    "#seqSize = 140\n",
    "\n",
    "#save preprocessing results and model weights\n",
    "useSaves = False\n",
    "\n",
    "#number of symbolics for SAX\n",
    "n_bins = 5\n",
    "\n",
    "#Load and formate data\n",
    "data_train, meta_train = arff.loadarff(data_path_train)\n",
    "data_test, meta_test = arff.loadarff(data_path_test)\n",
    "\n",
    "data_train = np.array(data_train.tolist())\n",
    "data_test = np.array(data_test.tolist())\n",
    "\n",
    "y_trainy = data_train[:,-1].astype(int)\n",
    "y_train = []\n",
    "X_train = data_train[:,:-1]\n",
    "y_testy_full = data_test[:,-1].astype(int)\n",
    "y_testy = y_testy_full\n",
    "y_test = []\n",
    "X_test = data_test[:,:-1]\n",
    "\n",
    "X_train, y_trainy = shuffle(X_train, y_trainy, random_state = seed_value)\n",
    "\n",
    "for y in y_trainy:\n",
    "    y_train_puffer = np.zeros(num_of_classes)\n",
    "    y_train_puffer[y-1] = 1\n",
    "    y_train.append(y_train_puffer)\n",
    "\n",
    "for y in y_testy:\n",
    "    y_puffer = np.zeros(num_of_classes)\n",
    "    y_puffer[y-1] = 1\n",
    "    y_test.append(y_puffer)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.astype(float)\n",
    "y_test_full = np.array(y_test)\n",
    "y_test_full = y_test_full.astype(float)\n",
    "y_test = y_test_full  \n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "X_train = X_train.astype(float)\n",
    "\n",
    "# Initialize k-folds\n",
    "kf = StratifiedKFold(5, shuffle=True, random_state=seed_value) # Use for StratifiedKFold classification\n",
    "fold = 0\n",
    "\n",
    "# Earlystopping callback\n",
    "earlystop = EarlyStopping(monitor= 'val_loss', min_delta=0 , patience=50, verbose=0, mode='auto')\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots to exam the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = 1\n",
    "\n",
    "for z in range(15):\n",
    "\n",
    "    x = range(seqSize)\n",
    "    y = X_train[z]\n",
    "\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    print(y_train[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the transformer model with given information\n",
    "def createModel(splits, x_train, x_val, x_test, batchSize, num_of_classes, doMask= False, rate = 0, numOfAttentionLayers=2):    \n",
    "        print(np.array(x_train1).shape)\n",
    "        x_trains = np.dsplit(x_train, splits)\n",
    "        print(np.array(x_trains).shape)\n",
    "        #x_trains = np.array(x_trains).squeeze(axis=3)\n",
    "        x_trainsBatch = np.dsplit(x_train[:batchSize], splits)\n",
    "\n",
    "        x_tests = np.dsplit(x_test, splits)\n",
    "        x_vals = np.dsplit(x_val, splits)\n",
    "        maxLen = len(x_trains[0][0])\n",
    "        print(maxLen)\n",
    "\n",
    "        print(np.array(x_trains).shape)\n",
    "        flattenArray = []\n",
    "        inputShapes = []\n",
    "        encClasses = []\n",
    "        for i in range(len(x_trains)):\n",
    "            if doMask:\n",
    "\n",
    "                masky = createMask(x_trains[i], 6)\n",
    "                x_part = np.array(x_trains[i])\n",
    "                print(np.array(x_part).shape)\n",
    "            \n",
    "                seq_len1 = x_part.shape[1]\n",
    "\n",
    "                sens1 = x_part.shape[2]\n",
    "                input_shape1 = (seq_len1, sens1)\n",
    "                left_input1 = tf.keras.layers.Input(input_shape1, name='input_ids')\n",
    "                    \n",
    "                mask = tf.keras.layers.Input(shape=masky.shape[1:], name='attention_mask')\n",
    "                print('masky shape')\n",
    "                print(masky.shape)\n",
    "                print(mask)\n",
    "            else: \n",
    "                mask = Input(1)\n",
    "                x_part = np.array(x_trains[i])\n",
    "                print(np.array(x_part).shape)\n",
    "            \n",
    "                seq_len1 = x_part.shape[1]\n",
    "\n",
    "                sens1 = x_part.shape[2]\n",
    "                input_shape1 = (seq_len1, sens1)\n",
    "                left_input1 = Input(input_shape1)\n",
    "\n",
    "            inputShapes.append(left_input1)\n",
    "            if doMask:\n",
    "                inputShapes.append(mask)\n",
    "\n",
    "            encoded = left_input1\n",
    "            input_vocab_size = 0\n",
    "            \n",
    "            #create transformer encoder layer \n",
    "            encClass1 = transformer.Encoder(numOfAttentionLayers, 16, 6, 6, 5000, rate=rate, input_vocab_size = input_vocab_size + 2, maxLen = maxLen, doMask=doMask, seed_value=seed_value)\n",
    "                \n",
    "            encClasses.append(encClass1)\n",
    "            if doMask:\n",
    "                encInput = encoded, mask\n",
    "                print('ssssssssssssssssssssss')\n",
    "                print(encoded.shape)\n",
    "                print(mask.shape)\n",
    "                print(encoded)\n",
    "                print(mask)\n",
    "                print('endddddddd')\n",
    "            else:\n",
    "                maskLayer = tf.keras.layers.Masking(mask_value=-2)\n",
    "                encInput = maskLayer(encoded)\n",
    "            enc1, attention, fullAttention = encClass1(encInput)\n",
    "            flatten1 = Flatten()(enc1)\n",
    "            flattenArray.append(flatten1)\n",
    "        \n",
    "\n",
    "        # Merge nets\n",
    "        if splits == 1:\n",
    "            merged = flattenArray[0]\n",
    "        else:\n",
    "            merged = concatenate(flattenArray)\n",
    "\n",
    "        output = Dense(num_of_classes, activation = \"sigmoid\")(merged)\n",
    "        \n",
    "        # Create combined model\n",
    "        wdcnnt_multi = Model(inputs=inputShapes,outputs=(output))\n",
    "        print(wdcnnt_multi.summary())\n",
    "        \n",
    "        print(wdcnnt_multi.count_params())\n",
    "        \n",
    "        tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=seed_value)\n",
    "\n",
    "        learning_rate = transformer.CustomSchedule(16)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.99, \n",
    "                                     epsilon=1e-9)\n",
    "        \n",
    "        wdcnnt_multi.compile(optimizer=optimizer,\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['accuracy'], run_eagerly=False)\n",
    "        \n",
    "        print('done')\n",
    "        \n",
    "        return wdcnnt_multi, inputShapes, x_trains, x_tests, x_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstracting dating with interpolation\n",
    "def abstractData(data, earlyPredictorZ, takeAvg = True, heatLayer = 0, limit = 300):\n",
    "    attentionFQ = []\n",
    "    \n",
    "    #split data so only #limit many variables are predicted at a time\n",
    "    for bor in range(int(math.ceil(len(data)/limit))):\n",
    "        if(takeAvg):\n",
    "            attentionFQ.extend(earlyPredictorZ.predict([data[bor*limit:(bor+1)*limit]])[1])\n",
    "        else: \n",
    "            attentionFQ.extend(earlyPredictorZ.predict([data[bor*limit:(bor+1)*limit]])[2])\n",
    "    newX = []\n",
    "    reduction = []\n",
    "    for index in range(len(data)):        \n",
    "\n",
    "            X_sax = np.array(data).squeeze()[index]\n",
    "            X_ori = X_sax \n",
    "\n",
    "            if(takeAvg):\n",
    "                heat = np.sum(np.max(attentionFQ[index], axis = 1), axis = 0)\n",
    "            else:\n",
    "                heat = np.sum(np.max(attentionFQ[heatLayer][index], axis = 1), axis = 0)\n",
    "        \n",
    "            #Do abstraction based on thresholds\n",
    "            if doMax:\n",
    "                maxHeat = np.max(heat)\n",
    "                borderHeat = maxHeat/2\n",
    "                borderHeat2 = maxHeat/3\n",
    "            else:\n",
    "                maxHeat = np.average(heat)\n",
    "                borderHeat = maxHeat\n",
    "                borderHeat2 = maxHeat/1.2\n",
    "\n",
    "            #filter based on heat\n",
    "            fitleredSet = []\n",
    "            indexSet = []\n",
    "            avgSet = []\n",
    "            for h in range(len(heat)):\n",
    "                if heat[h] > borderHeat:\n",
    "                    if len(avgSet) != 0:\n",
    "                        fitleredSet.append(np.median(avgSet))\n",
    "                        indexSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                        avgSet = []\n",
    "                    fitleredSet.append(X_ori[h])\n",
    "                    indexSet.append(h)\n",
    "                elif heat[h] > borderHeat2:\n",
    "                    avgSet.append(X_ori[h])\n",
    "                elif len(avgSet) != 0:\n",
    "                    fitleredSet.append(np.median(avgSet))\n",
    "                    indexSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                    avgSet = []\n",
    "            \n",
    "            if len(avgSet) != 0:\n",
    "                fitleredSet.append(np.median(avgSet))\n",
    "                indexSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                \n",
    "            reduction.append(1 - len(fitleredSet)/len(heat))\n",
    "            \n",
    "            if(len(fitleredSet) < 2):\n",
    "                fitleredSet.append(0)\n",
    "                indexSet.append(len(heat))\n",
    "            newXTemp = interp1d(indexSet, fitleredSet, bounds_error = False, fill_value = -2)\n",
    "            newX.append([[x] for x in newXTemp(range(len(heat)))])\n",
    "\n",
    "    newX = np.array(newX, dtype=np.float32)\n",
    "    print(np.array(newX).shape)\n",
    "    print(data.shape)\n",
    "    return newX, reduction\n",
    "\n",
    "#abstracting dating with no interpolation and added mask values\n",
    "def abstractData2(data, earlyPredictorZ, takeAvg = True, heatLayer = 0, limit = 300):\n",
    "    attentionFQ = []\n",
    "    for bor in range(int(math.ceil(len(data)/limit))):\n",
    "        if(takeAvg):\n",
    "            attentionFQ.extend(earlyPredictorZ.predict([data[bor*limit:(bor+1)*limit]])[1])\n",
    "        else: \n",
    "            attentionFQ.extend(earlyPredictorZ.predict([data[bor*limit:(bor+1)*limit]])[2])\n",
    "    newX = []\n",
    "    reduction = []\n",
    "    for index in range(len(data)):        \n",
    "            X_sax = np.array(data).squeeze()[index]\n",
    "            X_ori = X_sax \n",
    "\n",
    "            if(takeAvg):\n",
    "                heat = np.sum(np.max(attentionFQ[index], axis = 1), axis = 0)\n",
    "            else:\n",
    "                heat = np.sum(np.max(attentionFQ[heatLayer][index], axis = 1), axis = 0)\n",
    "            if doMax:\n",
    "                maxHeat = np.max(heat)\n",
    "                borderHeat = maxHeat/2\n",
    "                borderHeat2 = maxHeat/3\n",
    "            else:\n",
    "                maxHeat = np.average(heat)\n",
    "                borderHeat = maxHeat\n",
    "                borderHeat2 = maxHeat/1.2\n",
    "            #print(heat)\n",
    "            #print(maxHeat)\n",
    "\n",
    "            \n",
    "            #filter based on heat and add masked values\n",
    "            fitleredSet = []\n",
    "            indexSet = []\n",
    "            avgSet = []\n",
    "            for h in range(len(heat)):\n",
    "                if heat[h] > borderHeat:\n",
    "                    if len(avgSet) != 0:\n",
    "                        fitleredSet[h - math.ceil(len(avgSet)/2)] = np.median(avgSet)\n",
    "                        avgSet = []\n",
    "                    fitleredSet.append(X_ori[h])\n",
    "                elif heat[h] > borderHeat2:\n",
    "                    fitleredSet.append(-2)\n",
    "                    avgSet.append(X_ori[h])\n",
    "                elif len(avgSet) != 0:\n",
    "                    fitleredSet.append(-2)\n",
    "                    fitleredSet[h - math.ceil(len(avgSet)/2)] = np.median(avgSet)\n",
    "\n",
    "                    avgSet = []\n",
    "                else:\n",
    "                    fitleredSet.append(-2)\n",
    "            if len(avgSet) != 0:\n",
    "                fitleredSet[h - math.ceil(len(avgSet)/2)] = np.median(avgSet)\n",
    "\n",
    "            reduction.append(1 - len([x for x in fitleredSet if x != -2])/len(heat))\n",
    "            newX.append([[x] for x in fitleredSet])\n",
    "\n",
    "\n",
    "    newX = np.array(newX, dtype=np.float32)\n",
    "    print(np.array(newX).shape)\n",
    "    print(data.shape)\n",
    "    return newX, reduction\n",
    "\n",
    "#preprocess data with StandardScaler and SAX\n",
    "def preprocessData(x_train1, x_val, X_test, y_train1, y_val, y_test, y_trainy, y_testy, binNr):    \n",
    "    \n",
    "    x_test = X_test.copy()\n",
    "    \n",
    "    processedDataName = \"./saves/\"+str(data_path_train.split('/')[-1].split('.')[0])+ '-size' + str(seqSize) + '-bin' + str(binNr)\n",
    "    fileExists = os.path.isfile(processedDataName +'.pkl')\n",
    "\n",
    "    if(fileExists and useSaves):\n",
    "        print('found file! Start loading file!')\n",
    "        res = helper.load_obj(processedDataName)\n",
    "\n",
    "\n",
    "        for index, v in np.ndenumerate(res):\n",
    "            print(index)\n",
    "            res = v\n",
    "        res.keys()\n",
    "\n",
    "        x_train1 = res['X_train']\n",
    "        x_train1 = res['X_val']\n",
    "        x_test = res['X_test']\n",
    "        x_val = res['X_val']\n",
    "        X_train_ori = res['X_train_ori']\n",
    "        X_test_ori = res['X_test_ori']\n",
    "        y_trainy = res['y_trainy']\n",
    "        y_train1 = res['y_train']\n",
    "        y_test = res['y_test']\n",
    "        y_testy = res['y_testy']\n",
    "        y_val = res['y_val']\n",
    "        X_val_ori = res['X_val_ori']\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "        print(y_test.shape)\n",
    "        print(y_train.shape)\n",
    "        \n",
    "    else:\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "        print(x_val.shape)\n",
    "        print(y_test.shape)\n",
    "        print(y_train.shape)\n",
    "        trainShape = x_train1.shape\n",
    "        valShape = x_val.shape\n",
    "        testShape = x_test.shape\n",
    "        \n",
    "        scaler = StandardScaler()    \n",
    "        scaler = scaler.fit(x_train1.reshape((-1,1)))\n",
    "        X_train = scaler.transform(x_train1.reshape(-1, 1)).reshape(trainShape)\n",
    "        x_val = scaler.transform(x_val.reshape(-1, 1)).reshape(valShape)\n",
    "        x_test = scaler.transform(x_test.reshape(-1, 1)).reshape(testShape)\n",
    "\n",
    "        X_test_ori = x_test.copy()\n",
    "        X_val_ori = x_val.copy()\n",
    "        X_train_ori = x_train1.copy()\n",
    "\n",
    "        #Do SAX\n",
    "        sax = SymbolicAggregateApproximation(n_bins=n_bins, strategy='uniform')\n",
    "        sax.fit(x_train1)\n",
    "        x_train1 = helper.symbolizeTrans(x_train1, sax)\n",
    "        x_val = helper.symbolizeTrans(x_val, sax)\n",
    "        x_test = helper.symbolizeTrans(x_test, sax)\n",
    "\n",
    "            \n",
    "\n",
    "        x_train1 = np.expand_dims(x_train1, axis=2)\n",
    "        x_val = np.expand_dims(x_val, axis=2)\n",
    "        x_test = np.expand_dims(x_test, axis=2)   \n",
    "        X_test_ori = np.expand_dims(X_test_ori, axis=2)   \n",
    "        X_train_ori = np.expand_dims(X_train_ori, axis=2) \n",
    "        X_val_ori = np.expand_dims(X_val_ori, axis=2) \n",
    "\n",
    "        print(x_test.shape)\n",
    "        print(x_train1.shape)\n",
    "\n",
    "        #save sax results to only calculate them once\n",
    "        resultsSave = {\n",
    "            'X_train':x_train1,\n",
    "            'X_train_ori':X_train_ori,\n",
    "            'X_test':x_test,\n",
    "            'X_test_ori':X_test_ori,\n",
    "            'X_val': x_val,\n",
    "            'X_val_ori':X_val_ori,\n",
    "            'y_trainy':y_trainy,\n",
    "            'y_train':y_train1,\n",
    "            'y_val': y_val,\n",
    "            'y_test':y_test,\n",
    "            'y_testy':y_testy\n",
    "        }\n",
    "        helper.save_obj(resultsSave, processedDataName)\n",
    "    return x_train1, x_val, x_test, y_train1, y_val, y_test, X_train_ori, X_val_ori, X_test_ori, y_trainy, y_testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get save weight names! Depends on other set variables\n",
    "def getWeightName(learning = True):\n",
    "    baseName = \"./saves/weights-\" + str(data_path_train.split('/')[-1].split('.')[0]) + '-size' + str(seqSize) + '-threshold' + maxString + '-input' + abstractionString + '-fold' + str(fold)\n",
    "    if learning:\n",
    "        return baseName + '-learning.tf'\n",
    "    else:\n",
    "        return baseName + '.tf'\n",
    "\n",
    "#do training with abstraction level = abstraction\n",
    "def doAbstractedTraining(trainD, valD, testD, abstraction = 0, earlyPredictorZ = None, takeAvg = True, rate=0, heatLayer = 0, numOfAttentionLayers = 1):\n",
    "    \n",
    "    #Interpolation abstraction\n",
    "    if abstraction == 2 and earlyPredictorZ != None:\n",
    "        newTrain, trainReduction = abstractData(trainD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "        newVal, valReduction = abstractData(valD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "        newTest, testReduction = abstractData(testD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "    #Mask abstraction\n",
    "    elif abstraction == 3 and earlyPredictorZ != None:\n",
    "        newTrain, trainReduction = abstractData2(trainD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "        newVal, valReduction = abstractData2(valD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "        newTest, testReduction = abstractData2(testD, earlyPredictorZ, takeAvg = takeAvg, heatLayer = heatLayer)\n",
    "    else:\n",
    "        newTrain = trainD\n",
    "        newVal = valD\n",
    "        newTest = testD\n",
    "        trainReduction = 0\n",
    "        valReduction = 0\n",
    "        testReduction = 0\n",
    "    \n",
    "\n",
    "    #calclulate shifts of the data\n",
    "    valShifts = []\n",
    "    smallerValSet = []\n",
    "    for val in newVal:\n",
    "        shifts = -1\n",
    "        smallerSet = 2\n",
    "        lastVal = val[0][0]\n",
    "        rise = -3\n",
    "        timeSkip = 1\n",
    "        for v in val[1:]:\n",
    "            v = v[0]\n",
    "\n",
    "            if v == -2:\n",
    "                timeSkip += 1\n",
    "            elif helper.truncate((v - lastVal) / timeSkip) != rise:\n",
    "                shifts += 1\n",
    "                smallerSet += 1\n",
    "                rise = helper.truncate((v - lastVal) / timeSkip)\n",
    "                lastVal = v\n",
    "                timeSkip = 1\n",
    "            else:\n",
    "                lastVal = v\n",
    "        valShifts.append(shifts)\n",
    "        smallerValSet.append(smallerSet)\n",
    "\n",
    "    valShifts = np.average(valShifts)\n",
    "    smallerValSet = np.average(smallerValSet)\n",
    "\n",
    "\n",
    "    testShifts = []\n",
    "    smallerTestSet = []\n",
    "    print(newTest.shape)\n",
    "    for val in newTest:\n",
    "        shifts = -1\n",
    "        smallerSet = 2\n",
    "        lastVal = val[0][0]\n",
    "        rise = -3\n",
    "        timeSkip = 1\n",
    "        for v in val[1:]:\n",
    "            v = v[0]\n",
    "            if v == -2:\n",
    "                timeSkip += 1\n",
    "            elif round(v - lastVal / timeSkip, 1) != rise:\n",
    "                shifts += 1\n",
    "                smallerSet += 1\n",
    "                rise = round(v - lastVal / timeSkip, 1)\n",
    "                lastVal = v\n",
    "                timeSkip = 1\n",
    "            else:\n",
    "                lastVal = v\n",
    "        testShifts.append(shifts)\n",
    "        smallerTestSet.append(smallerSet)\n",
    "\n",
    "    testShifts = np.average(testShifts)\n",
    "    smallerTestSet = np.average(smallerTestSet)\n",
    "\n",
    "    #create model and train\n",
    "    n_model2, inputs2, x_trains2, x_tests2, x_vals2 = createModel(1, newTrain, newVal, newTest , BATCH, num_of_classes, rate=rate, doMask=False, numOfAttentionLayers=numOfAttentionLayers)\n",
    "    weightsName = getWeightName(learning=True)\n",
    "    saveBest2 = transformer.SaveBest(weightsName)\n",
    "    print(np.array(x_trains2).shape)\n",
    "    print(np.array(x_vals2).shape)\n",
    "\n",
    "    x_trains_mask = x_trains2\n",
    "    \n",
    "    if (os.path.isfile(getWeightName(learning=False) + '.index') and useSaves):\n",
    "        print('found weights to load! Won\\'t train model!')\n",
    "        n_model2.load_weights(getWeightName(learning=False))\n",
    "    else:\n",
    "        print('No weights found! Start training model!')\n",
    "        n_model2.fit(x_trains_mask, y_train1, validation_data = (x_vals2, y_val) , epochs = 500, batch_size = BATCH, verbose=1, callbacks =[earlystop, saveBest2], shuffle = True)\n",
    "        n_model2.load_weights(getWeightName(learning=True))\n",
    "        n_model2.save_weights(getWeightName(learning=False), overwrite=True)\n",
    "        \n",
    "    earlyPredictor2 = tf.keras.Model(n_model2.inputs, n_model2.layers[2].output)\n",
    "\n",
    "    # Predictions on the validation set\n",
    "    predictions2 = n_model2.predict(x_vals2)\n",
    "    attentionQ2 = earlyPredictor2.predict(x_vals2)\n",
    "\n",
    "    print('############################')\n",
    "    predictions2 = np.argmax(predictions2,axis=1)\n",
    "\n",
    "    # Measure this fold's accuracy on validation set compared to actual labels\n",
    "    y_compare = np.argmax(y_val, axis=1)\n",
    "    val_score2 = metrics.accuracy_score(y_compare, predictions2)\n",
    "\n",
    "    print(f\"validation fold score with input {abstractionString}-{maxString}(accuracy): {val_score2}\")\n",
    "\n",
    "    # Predictions on the test set\n",
    "    limit = 300\n",
    "    test_predictions_loop2 = []\n",
    "    for bor in range(int(math.ceil(len(x_tests2[0])/limit))):\n",
    "        test_predictions_loop2.extend(n_model2.predict([x_tests2[0][bor*limit:(bor+1)*limit]]))\n",
    "\n",
    "    # Append actual labels of the test set to empty list\n",
    "    y_testyy = [y-1 for y in y_testy]\n",
    "    test_predictions_loop2 = np.argmax(test_predictions_loop2, axis=1)\n",
    "\n",
    "    # Measure this fold's accuracy on test set compared to actual labels\n",
    "    test_score2 = metrics.accuracy_score(y_testyy, test_predictions_loop2)\n",
    "\n",
    "    print(f\"test fold score with input {abstractionString}-{maxString}(accuracy): {test_score2}\")\n",
    "    return val_score2, test_score2, predictions2, test_predictions_loop2, n_model2, inputs2, x_trains2, x_tests2, x_vals2, attentionQ2, smallerValSet, smallerTestSet, valShifts, testShifts, earlyPredictor2, newTrain, newVal, newTest, valReduction, testReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize loop for every kth fold\n",
    "\n",
    "doAbstraction = True\n",
    "#Attention layers count\n",
    "numOfAttentionLayers = 2\n",
    "#take Attention average\n",
    "takeAvg = True\n",
    "#drouput rate\n",
    "rate=0.3\n",
    "\n",
    "usedAbstraction = ['Ori', 'SAX', 'interpol', 'mask']    \n",
    "usedThresholds = ['None', 'None', 'average', 'max']   \n",
    "BATCH = 50\n",
    "fold = 0\n",
    "\n",
    "maxString = 'None'\n",
    "accResults = [[],[],[],[],[],[]]\n",
    "resultNames = ['Ori', 'SAX', 'avgInter', 'maxInter', 'avgMask', 'maxMask' ]\n",
    "\n",
    "for train, test in kf.split(X_train, y_trainy): # Must specify y StratifiedKFold for \n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    \n",
    "    #preprocess data\n",
    "    x_train1 = X_train[train]\n",
    "    x_val = X_train[test]\n",
    "    y_train1 = y_train[train]\n",
    "    y_val = y_train[test]\n",
    "    \n",
    "    x_train1, x_val, x_test, y_train1, y_val, y_test, X_train_ori, X_val_ori, X_test_ori, y_trainy, y_testy = preprocessData(x_train1, x_val, X_test, y_train1, y_val, y_test, y_trainy, y_testy, fold)\n",
    "\n",
    "    abstractionIndex = 0\n",
    "    resultIndex = 0\n",
    "    \n",
    "    #ori data    \n",
    "    abstractionString = usedAbstraction[abstractionIndex]    \n",
    "    outOri = doAbstractedTraining(X_train_ori, X_val_ori, X_test_ori, abstractionIndex, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "\n",
    "    accResults[resultIndex].append(outOri)\n",
    "    resultIndex+=1\n",
    "    \n",
    "    # sax data    \n",
    "    abstractionIndex += 1\n",
    "    abstractionString = usedAbstraction[abstractionIndex]  \n",
    "    outSax = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "    accResults[resultIndex].append(outSax)\n",
    "    resultIndex+=1\n",
    "    \n",
    "    if doAbstraction:\n",
    "        earlyPredictor = outSax[-6]\n",
    "        abstractionIndex += 1\n",
    "        abstractionString = usedAbstraction[abstractionIndex]  \n",
    "        doMax = False\n",
    "        maxString = 'average'\n",
    "        outAvgAverage = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, earlyPredictorZ=earlyPredictor, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "        accResults[resultIndex].append(outAvgAverage)\n",
    "        resultIndex+=1        \n",
    "        doMax = True\n",
    "        maxString = 'max'\n",
    "        outMaxAverage = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, earlyPredictorZ=earlyPredictor, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "        accResults[resultIndex].append(outMaxAverage)\n",
    "        resultIndex+=1    \n",
    "        \n",
    "        abstractionIndex += 1\n",
    "        abstractionString = usedAbstraction[abstractionIndex]  \n",
    "        doMax = False\n",
    "        maxString = 'average'\n",
    "        outAvgMask = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, earlyPredictorZ=earlyPredictor, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "        accResults[resultIndex].append(outAvgMask)\n",
    "        resultIndex+=1         \n",
    "        doMax = True\n",
    "        maxString = 'max'\n",
    "        outMaxMask = doAbstractedTraining(x_train1, x_val, x_test, abstractionIndex, earlyPredictorZ=earlyPredictor, rate=rate, takeAvg = takeAvg, heatLayer = 0, numOfAttentionLayers = numOfAttentionLayers)   \n",
    "        accResults[resultIndex].append(outMaxMask)\n",
    "        resultIndex+=1     \n",
    "    \n",
    "rIndex = 0    \n",
    "for results in accResults:\n",
    "    resultName = resultNames[rIndex]\n",
    "    rIndex += 1\n",
    "    \n",
    "    print('#########################################')\n",
    "    print(resultName + ' Scores:')\n",
    "    print('#########################################')\n",
    "    print(f\"Avg validation score (accuracy): {np.average([r[0] for r in results])}\")   \n",
    "    print(f\"Avg test score (accuracy): {np.average([r[1] for r in results])}\")\n",
    "    print(f\"Avg Val reduced by: {np.average([r[-2] for r in results])}\")\n",
    "    print(f\"Avg Test reduced by: {np.average([r[-1] for r in results])}\")\n",
    "    print(f\"Avg Interpol Val Size: {np.average([r[-10] for r in results])}\")\n",
    "    print(f\"Avg Interpol Test Size: {np.average([r[-8] for r in results])}\")\n",
    "    print(f\"Avg Val shifts: {np.average([r[-8] for r in results])}\")\n",
    "    print(f\"Avg Test shifts: {np.average([r[-7] for r in results])}\")\n",
    "\n",
    "    if rIndex > 2:\n",
    "        abstractValPred = np.concatenate([r[2] for r in results])\n",
    "        oriValPred = np.concatenate([r[2] for r in accResults[1]])\n",
    "        valSwitch = 0\n",
    "        for i in range(len(abstractValPred)):\n",
    "            if abstractValPred[i] != oriValPred[i]:\n",
    "                valSwitch += 1\n",
    "                              \n",
    "        abstractTestPred = np.concatenate([r[3] for r in results])\n",
    "        oriTestPred = np.concatenate([r[3] for r in accResults[1]])\n",
    "        testSwitch = 0\n",
    "        for i in range(len(abstractTestPred)):\n",
    "            if abstractTestPred[i] != oriTestPred[i]:\n",
    "                testSwitch += 1\n",
    "\n",
    "        print(f\"Val Switched: {valSwitch / len(abstractValPred)}\")\n",
    "        print(f\"Test Switched: {testSwitch / len(abstractTestPred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "heads = 6\n",
    "\n",
    "sns.set()\n",
    "predictions = outSax[2]\n",
    "attentionQ = outSax[9]\n",
    "print('###')\n",
    "print(y_val[index])\n",
    "print(predictions[index] + 1)\n",
    "for head in range(heads):\n",
    "    data_word = np.array(x_val).squeeze()[index]\n",
    "    data_att = np.max(attentionQ[1][index][head], axis = 0)\n",
    "    d = pd.DataFrame(data = data_att,index = data_word, columns=range(1))\n",
    "    f, ax = plt.subplots(figsize=(60,3))\n",
    "    d = d.transpose()\n",
    "    sns.heatmap(d, vmin=0, vmax=0.1, ax=ax, cmap=\"OrRd\")\n",
    "    label_y = ax.get_yticklabels()\n",
    "    plt.setp(label_y, rotation=360, horizontalalignment='right')\n",
    "    label_x = ax.get_xticklabels()\n",
    "    plt.setp(label_x, rotation=45, horizontalalignment='right')\n",
    "    plt.tick_params(labelsize=26)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare attention abstraction data for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "ran = 60\n",
    "\n",
    "maxLen = 0\n",
    "predictionsT = outAvgAverage[2]\n",
    "attentionQT = outAvgAverage[9]\n",
    "abstractedData = outAvgAverage[-4]\n",
    "\n",
    "filteredTrain = []\n",
    "filteredVal = []\n",
    "for indi in range(len(abstractedData)):\n",
    "\n",
    "    X_sax = np.array(abstractedData).squeeze()[indi]\n",
    "    X_ori = X_sax\n",
    "\n",
    "    heat = np.sum(np.max(attentionQT[1][indi], axis = 1), axis = 0)\n",
    "    maxHeat = np.average(heat)\n",
    "    borderHeat = maxHeat\n",
    "\n",
    "\n",
    "    fitleredSet = []\n",
    "    for h in range(len(heat)):\n",
    "        if heat[h] > borderHeat:\n",
    "            fitleredSet.append([X_ori[h]])\n",
    "    filteredTrain.append(fitleredSet)\n",
    "    if len(fitleredSet) > maxLen:\n",
    "        maxLen = len(fitleredSet)\n",
    "        \n",
    "print(\"++++\")\n",
    "x_vals = x_val\n",
    "for indi in range(len(x_vals[0])):\n",
    "\n",
    "    X_sax = np.array(x_vals).squeeze()[indi]\n",
    "    X_ori = X_sax\n",
    "\n",
    "    heat = np.sum(np.max(attentionQ[1][indi], axis = 1), axis = 0)\n",
    "    maxHeat = np.average(heat)\n",
    "    borderHeat = maxHeat / 2\n",
    "\n",
    "\n",
    "    fitleredSet = []\n",
    "    for h in range(len(heat)):\n",
    "        if heat[h] > borderHeat:\n",
    "            fitleredSet.append([X_ori[h]])\n",
    "    filteredVal.append(fitleredSet)\n",
    "    if len(fitleredSet) > maxLen:\n",
    "        maxLen = len(fitleredSet)\n",
    "        \n",
    "print(\"####\")\n",
    "print(maxLen)\n",
    "print(np.array(filteredVal).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show attention abstraction for each head for a single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ran = 60\n",
    "# Parameters\n",
    "X_sax = np.array(abstractedData).squeeze()[index]\n",
    "X_ori = X_sax \n",
    "\n",
    "# Compute gaussian bins\n",
    "bins = norm.ppf(np.linspace(0, 1, n_bins + 1)[1:-1])\n",
    "\n",
    "# Show the results for the first time series\n",
    "bottom_bool = X_sax\n",
    "#heads = 6\n",
    "for head in range(heads):\n",
    "\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    plt.plot(X_ori, 'o--', label='Original')\n",
    "    for x, y, s, bottom in zip(range(ran), X_ori, X_sax, bottom_bool):\n",
    "        va = 'bottom' if bottom else 'top'\n",
    "        plt.text(x, y, s, ha='center', va=va, fontsize=14, color='#ff7f0e')\n",
    "    plt.hlines(bins, 0, ran, color='g', linestyles='--', linewidth=0.5)\n",
    "    sax_legend = mlines.Line2D([], [], color='#ff7f0e', marker='*',\n",
    "                               label='SAX - {0} bins'.format(n_bins))\n",
    "    first_legend = plt.legend(handles=[sax_legend], fontsize=8, loc=(0.76, 0.86))\n",
    "    ax = plt.gca().add_artist(first_legend)\n",
    "    plt.legend(loc=(0.81, 0.93), fontsize=8)\n",
    "    plt.xlabel('Time', fontsize=14)\n",
    "    plt.title('Symbolic Aggregate approXimation', fontsize=16)\n",
    "    heat = np.max(attentionQ[1][index][head], axis = 0)\n",
    "\n",
    "    for i in range(len(heat)):\n",
    "        plt.axvspan(i, i+1, color='red', alpha=heat[i]*6)\n",
    "    plt.show()\n",
    "    \n",
    "    maxHeat = np.average(heat)\n",
    "    borderHeat = maxHeat\n",
    "    borderHeat2 = maxHeat/1.2\n",
    "\n",
    "    fitleredSet = []\n",
    "    timeSet = []\n",
    "    avgSet = []\n",
    "    for h in range(len(heat)):\n",
    "        if heat[h] > borderHeat:\n",
    "            if len(avgSet) != 0:\n",
    "                fitleredSet.append(np.median(avgSet))\n",
    "                timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                avgSet = []\n",
    "            fitleredSet.append(X_ori[h])\n",
    "            timeSet.append(h)\n",
    "        elif heat[h] > borderHeat2:\n",
    "            avgSet.append(X_ori[h])\n",
    "            #avgSet = []\n",
    "        elif len(avgSet) != 0:\n",
    "            fitleredSet.append(np.median(avgSet))\n",
    "            timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "            avgSet = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 2))\n",
    "    plt.plot(timeSet, fitleredSet, 'o--', label='Original')\n",
    "    for x, y, s, bottom in zip(range(len(heat)), fitleredSet, fitleredSet, bottom_bool):\n",
    "        va = 'bottom' if bottom else 'top'\n",
    "    plt.hlines(bins, 0, len(heat), color='g', linestyles='--', linewidth=0.5)\n",
    "    sax_legend = mlines.Line2D([], [], color='#ff7f0e', marker='*',\n",
    "                               label='SAX - {0} bins'.format(n_bins))\n",
    "    first_legend = plt.legend(handles=[sax_legend], fontsize=8, loc=(0.76, 0.86))\n",
    "    ax = plt.gca().add_artist(first_legend)\n",
    "    plt.legend(loc=(0.81, 0.93), fontsize=8)\n",
    "    plt.xlabel('Time', fontsize=14)\n",
    "    plt.title('Symbolic Aggregate approXimation', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "print('#################################################')\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(X_ori, 'o--', label='Original')\n",
    "for x, y, s, bottom in zip(range(ran), X_ori, X_sax, bottom_bool):\n",
    "    va = 'bottom' if bottom else 'top'\n",
    "    plt.text(x, y, s, ha='center', va=va, fontsize=14, color='#ff7f0e')\n",
    "plt.hlines(bins, 0, ran, color='g', linestyles='--', linewidth=0.5)\n",
    "sax_legend = mlines.Line2D([], [], color='#ff7f0e', marker='*',\n",
    "                           label='SAX - {0} bins'.format(n_bins))\n",
    "first_legend = plt.legend(handles=[sax_legend], fontsize=8, loc=(0.76, 0.86))\n",
    "ax = plt.gca().add_artist(first_legend)\n",
    "plt.legend(loc=(0.81, 0.93), fontsize=8)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.title('Symbolic Aggregate approXimation', fontsize=16)\n",
    "#heat = np.max(attentionQ[1][index][head], axis = 0)\n",
    "\n",
    "heat = np.sum(np.max(attentionQ[1][index], axis = 1), axis = 0)\n",
    "#heat = np.sum(np.max(attentionQ[2][1][index], axis = 1), axis = 0)\n",
    "maxHeat = np.average(heat)\n",
    "borderHeat = maxHeat\n",
    "borderHeat2 = maxHeat/1.2\n",
    "print(heat)\n",
    "print(maxHeat)\n",
    "\n",
    "#remove?\n",
    "for i in range(len(heat)):\n",
    "    plt.axvspan(i, i+1, color='red', alpha=heat[i]*1)\n",
    "plt.show()\n",
    "\n",
    "fitleredSet = []\n",
    "timeSet = []\n",
    "avgSet = []\n",
    "for h in range(len(heat)):\n",
    "    if heat[h] > borderHeat:\n",
    "        if len(avgSet) != 0:\n",
    "            fitleredSet.append(np.median(avgSet))\n",
    "            timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "            avgSet = []\n",
    "        fitleredSet.append(X_ori[h])\n",
    "        timeSet.append(h)\n",
    "    elif heat[h] > borderHeat2:\n",
    "        avgSet.append(X_ori[h])\n",
    "        #avgSet = []\n",
    "    elif len(avgSet) != 0:\n",
    "        fitleredSet.append(np.median(avgSet))\n",
    "        timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "        avgSet = []\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "plt.plot(timeSet, fitleredSet, 'o--', label='Original')\n",
    "for x, y, s, bottom in zip(range(len(heat)), fitleredSet, fitleredSet, bottom_bool):\n",
    "    va = 'bottom' if bottom else 'top'\n",
    "plt.hlines(bins, 0, len(heat), color='g', linestyles='--', linewidth=0.5)\n",
    "sax_legend = mlines.Line2D([], [], color='#ff7f0e', marker='*',\n",
    "                           label='SAX - {0} bins'.format(n_bins))\n",
    "first_legend = plt.legend(handles=[sax_legend], fontsize=8, loc=(0.76, 0.86))\n",
    "ax = plt.gca().add_artist(first_legend)\n",
    "plt.legend(loc=(0.81, 0.93), fontsize=8)\n",
    "plt.xlabel('Time', fontsize=14)\n",
    "plt.title('Symbolic Aggregate approXimation', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots original data, sax data and abstracted data for each lable in goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for goal in [1,2,3,4,5]:\n",
    "indexSafe = index\n",
    "\n",
    "#theSet = outAvgAverage[-4]\n",
    "theSet = outSax[7]\n",
    "valSet = y_test #y_val\n",
    "n_model = outSax[4]\n",
    "earlyPredictor = outSax[-6]\n",
    "doMax = False\n",
    "maxString = 'average'\n",
    "X_ori = outSax[7]\n",
    "\n",
    "limit = 300\n",
    "predictionsX = outSax[3]\n",
    "attentionQX = []\n",
    "for bor in range(int(math.ceil(len(theSet[0])/limit))):\n",
    "    attentionQX.extend(earlyPredictor.predict([theSet[0][bor*limit:(bor+1)*limit]])[1])\n",
    "\n",
    "counterLimit = 20\n",
    "for goal in [6]:\n",
    "    print(goal)\n",
    "\n",
    "    counter = 0 \n",
    "    for index in range(len(predictionsX)):\n",
    "        if predictionsX[index] + 1 == goal and np.argmax(valSet[index]) + 1 == goal and counter <= counterLimit:\n",
    "            counter += 1\n",
    "\n",
    "            X_sax = np.array(theSet).squeeze()[index]\n",
    "            X_ori = X_sax \n",
    "\n",
    "            plt.figure(figsize=(12, 2))\n",
    "            plt.plot(X_test_ori[index], 'o--', label='Original')\n",
    "            plt.title('Original Time Series', fontsize=16)\n",
    "            plt.xlabel('Time', fontsize=14)\n",
    "            #plt.savefig('./Bilder/' +  str(data_path_train.split('/')[-1].split('.')[0]) + '/interpolate/' + maxString + '/abstractions/' + str(valSet[index]) + '-p' + str(goal) + '-' + str(index) + 'oriForm.png', dpi = 300)\n",
    "            plt.show()\n",
    "            \n",
    "            print('#################################################')\n",
    "            print(np.argmax(valSet[index], axis=0))\n",
    "            plt.figure(figsize=(12, 2))\n",
    "            plt.plot(X_ori, 'o--', label='Original')\n",
    "            for x, y, s, bottom in zip(range(len(X_ori)), X_ori, X_sax, bottom_bool):\n",
    "                va = 'bottom' if bottom else 'top'\n",
    "            plt.hlines(bins, 0, ran, color='g', linestyles='--', linewidth=0.5)\n",
    "\n",
    "            plt.xlabel('Time', fontsize=14)\n",
    "            plt.title('Symbolic Time Series', fontsize=16)\n",
    "\n",
    "\n",
    "            heat = np.sum(np.max(attentionQX[index], axis = 1), axis = 0)\n",
    "            if doMax:\n",
    "                maxHeat = np.max(heat)\n",
    "                borderHeat = maxHeat/2\n",
    "                borderHeat2 = maxHeat/3\n",
    "            else:\n",
    "                maxHeat = np.average(heat)\n",
    "                borderHeat = maxHeat\n",
    "                borderHeat2 = maxHeat/1.2\n",
    "\n",
    "            #plt.savefig('./Bilder/' +  str(data_path_train.split('/')[-1].split('.')[0]) + '/interpolate/' + maxString + '/abstractions/' + str(valSet[index]) + '-p' + str(goal) + '-' + str(index) + 'saxForm.png', dpi = 300)\n",
    "            plt.show()\n",
    "            \n",
    "            data_att = heat\n",
    "            #d = pd.DataFrame(data = data_att,index = data_word, columns=data_word)\n",
    "            d = pd.DataFrame(data = data_att,index = data_word, columns=range(1))\n",
    "            f, ax = plt.subplots(figsize=(60,3))\n",
    "            d = d.transpose()\n",
    "            sns.heatmap(d, vmin=0, vmax=0.3, ax=ax, cmap=\"OrRd\")\n",
    "            label_y = ax.get_yticklabels()\n",
    "            plt.setp(label_y, rotation=360, horizontalalignment='right')\n",
    "            label_x = ax.get_xticklabels()\n",
    "            plt.setp(label_x, rotation=45, horizontalalignment='right')\n",
    "            plt.tick_params(labelsize=26)\n",
    "            #plt.savefig('./Bilder/' +  str(data_path_train.split('/')[-1].split('.')[0]) + '/interpolate/' + maxString + '/abstractions/' + str(valSet[index]) + '-p' + str(goal) + '-' + str(index) + 'heatForm.png', dpi = 300)\n",
    "            plt.show()\n",
    "\n",
    "            fitleredSet = []\n",
    "            timeSet = []\n",
    "            avgSet = []\n",
    "            for h in range(len(heat)):\n",
    "                if heat[h] > borderHeat:\n",
    "                    if len(avgSet) != 0:\n",
    "                        fitleredSet.append(np.median(avgSet))\n",
    "                        timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                        avgSet = []\n",
    "                    fitleredSet.append(X_ori[h])\n",
    "                    timeSet.append(h)\n",
    "                elif heat[h] > borderHeat2:\n",
    "                    avgSet.append(X_ori[h])\n",
    "                    #avgSet = []\n",
    "                elif len(avgSet) != 0:\n",
    "                    fitleredSet.append(np.median(avgSet))\n",
    "                    timeSet.append(h - math.ceil(len(avgSet)/2))\n",
    "                    avgSet = []\n",
    "\n",
    "            #print(fitleredSet)\n",
    "\n",
    "            plt.figure(figsize=(12, 2))\n",
    "            plt.plot(timeSet, fitleredSet, 'o--', label='Original')\n",
    "            for x, y, s, bottom in zip(range(len(heat)), fitleredSet, fitleredSet, bottom_bool):\n",
    "                va = 'bottom' if bottom else 'top'\n",
    "            plt.hlines(bins, 0, len(heat), color='g', linestyles='--', linewidth=0.5)\n",
    "            plt.xlabel('Time', fontsize=14)\n",
    "            plt.title('Abstracted Time Series', fontsize=16)\n",
    "            \n",
    "            #plt.savefig('./Bilder/' + str(data_path_train.split('/')[-1].split('.')[0]) + '/interpolate/' + maxString + '/abstractions/' + str(valSet[index]) + '-p' + str(goal) + '-' + str(index) + 'abstractForm.png', dpi = 300)\n",
    "            plt.show()\n",
    "\n",
    "index = indexSafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}